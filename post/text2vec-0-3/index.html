<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>text2vec 0.3  &middot; Data Science notes</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="text2vec, ">


<meta property="og:title" content="text2vec 0.3  &middot; Data Science notes ">
<meta property="og:site_name" content="Data Science notes"/>
<meta property="og:url" content="/post/text2vec-0-3/" />
<meta property="og:locale" content="en-us">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2016-03-17T00:00:00Z" />
<meta property="og:article:modified_time" content="2016-03-17T00:00:00Z" />

  
    
<meta property="og:article:tag" content="text2vec">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@dselivanov_" />
<meta name="twitter:creator" content="@dselivanov_" />
<meta name="twitter:title" content="text2vec 0.3" />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="/post/text2vec-0-3/" />
<meta name="twitter:domain" content="/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "text2vec 0.3",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2016-03-17",
    "description": "",
    "wordCount":  1696 
  }
</script>



<link rel="canonical" href="/post/text2vec-0-3/" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
<link rel="icon" href="/favicon.png">
<meta name="generator" content="Hugo 0.18" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.2/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->



    <link rel="stylesheet" href="/css/bootswatch/paper/bootstrap.min.css">


<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">


  <link rel="stylesheet" href="/css/highlight/default.css">


</head>
<body class="map[name:paper]" data-ng-app="myapp" data-ng-controller="MyController" data-ng-mouseleave="MouseLeave($event)">
    <header id="main-header">
  <nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        
          
          <a class="navbar-brand-img" href="/">
            <img alt="" src="">
            
          </a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav navbar-right">
            
            
            <li class="">

              <a href="/" >
                
                Blog
              </a>
            </li>
            
            <li class="">

              <a href="/projects" >
                
                Projects
              </a>
            </li>
            
            <li class="">

              <a href="/about" >
                
                About
              </a>
            </li>
            
          </ul>
        </div>
        
      </div>
    </nav>
  </header>


<div class="container">
  <div class="row">
    <div class="col-sm-9">
      <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
  <div class="text-center">

    <h1>text2vec 0.3
</h1>

    <div class="metas">
<small>
  <i class="fa fa-calendar"></i>
  <time datetime="2016-03-17">17 Mar, 2016</time>
</small>


  <small>
    &middot; by Dmitriy Selivanov
  
  &middot; Read in about 8 min
  &middot; (1696 words)
</small>


<div class="margin-10">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/text2vec" class="label label-primary">text2vec</a>
  


</div>
<br>
</div>

  </div>
</div>

      <div class="content">
  

<p><strong>updated 2016-03-31 - <a href="#updates">few functions renamed</a></strong></p>

<p><strong>updated 2016-10-07 - see <a href="http://dsnotes.com/articles/text2vec-0-4">updated tutorial for text2vec 0.4</a></strong></p>

<p>Today I&rsquo;m pleased to announce preview of the new version of text2vec. It is located in the 0.3 development branch, but very soon (probably in about a week) it will be merged into master.</p>

<p>To reproduce examples below, please install <code>text2vec@0.3</code> from github:</p>

<pre><code class="language-r">devtools::install_github('dselivanov/text2vec@0.3')
</code></pre>

<p>Also <strong>I&rsquo;m waiting for feedback from text2vec users, please spend a few minutes</strong>:</p>

<ol>
<li>What APIs are not clear / not intuitive?</li>
<li>What functionality is missing?</li>
<li>Do you have any problems with speed / RAM usage?</li>
</ol>

<h1 id="overview">Overview</h1>

<p><strong>In two words: <code>text2vec</code> became faster and more user-friendly</strong>. During the work on this version I almost didn&rsquo;t touch underlying core C++ code and focused on high-level features and usability. First I will briefly describe main improvements and then will provide full-featured example.</p>

<p>In this post i would like to highlight the following improvements:</p>

<ol>
<li>important <strong>bugfix</strong></li>
<li><code>dtm</code> keeps document ids as rownames</li>
<li>several <strong>API breaks</strong> - some functions removed, some renamed and some have another default arguments</li>
<li>performance improvements - <strong>all core functions have parallel mode</strong></li>
</ol>

<p>Full list of the features/changes available at github and marked with <a href="https://github.com/dselivanov/text2vec/milestones/0.3">0.3 tag</a>.</p>

<h2 id="bugfix">Bugfix</h2>

<p>There was one significant bug: when last document has no terms (at least from vocabulary), i.e. last row of <code>dtm</code> has all zeros, <code>get_dtm()</code> function omitted this last row. So <code>dtm</code> had less rows than number of documents in <code>corpus</code>. Now fixed.</p>

<h2 id="preserving-document-ids-in-corpus-and-dtm">Preserving document ids in <code>corpus</code> and <code>dtm</code></h2>

<p>I&rsquo;m not only the developer of the <code>text2vec</code>, but also probably the most active user. Since the first public release I felt that I needed to improve some rough edges. One of the most obviously missing things was lack of mechanism for keeping document <code>ids</code> during <code>corpus</code> (and <code>dtm</code>) construction. Now it is straightforward - if input of the <code>itoken</code> function has names, these names will be used as documents <code>ids</code>.</p>

<h2 id="new-high-level-api">New high-level API</h2>

<p>In 0.2 <code>corpus</code> was the central object. We can think about it as a container with reference semantics, which allow us to perform vectorization and collection of terms coocurence statistics <strong>simulteniously</strong>. After the corpus is created, only the following two functions are useful in 99% of cases - <code>get_dtm</code> and <code>get_tcm</code>. After that, users usually work with matrices. This means that <code>corpus</code> actually is an intermediate object and mainly should be used internally. In real life users usually need <em>Document-Term matrix (dtm)</em> <strong>or</strong> <em>Term-Cooccurence matrix (tcm)</em> which simplifies the process of transition from raw text to a vector space.</p>

<p>In 0.3 I introduce new higher-level API for direct <code>dtm</code> and <code>tcm</code> creation - <code>create_dtm()</code> and <code>create_tcm()</code> functions. Such simplification also allows me to implement efficient concurrent growing of <code>dtm</code> and <code>tcm</code>. <code>create_dtm()</code> and <code>create_tcm()</code> internally use <code>create_corpus()</code>, but hide all gory details and care about parallel execution. Experienced users, who need simulteniously vectorize corpus and collect cooccurence statistics, can still use <code>create_corpus()</code> and corresponding <code>get_dtm()</code>, <code>get_tcm</code> functions.</p>

<p>Another refinement - is the introduction of <code>vectorizer</code> concept. <code>vectorizer</code> is the function which performs mapping from raw text space to vector space. There are 2 kinds of vectorizers:</p>

<ol>
<li><code>vocab_vectorizer</code> which uses vocabulary to perfrom bag-of-ngrams vectorization;</li>
<li><code>hash_vectorizer</code> which uses feature hashing (or hashing trick);</li>
</ol>

<h2 id="iterators">Iterators</h2>

<p>As it was pointed out <a href="https://github.com/dselivanov/text2vec/issues/69">here</a>, in case of vocabulary vectorization, we perform 2 passes over input source. This means we read, preprocess and tokenize twice. While I/O usually is not an issue (if you use efficient reader like <code>data.table::fread</code> or functions from <code>readr</code> package), preprocessing can occupy a significant amount of time. For this reason I created <code>itoken</code> S3 method which works with <code>list</code> of <code>character</code> vectors - list of tokens. Now user can tokenize input and then reuse list of tokens in <code>create_vocabulary</code>, <code>dtm</code>, <code>tcm</code> construction. See examples below.</p>

<h2 id="vocabulary">Vocabulary</h2>

<p>There were several improvements to vocabulary construction:</p>

<ol>
<li>stopwords filtering during vocanulary construction (especially usefull for ngrams with <code>n &gt; 1</code>);</li>
<li><code>create_vocabulary</code> can be built in parallel using all your CPU cores;</li>
<li><code>prune_vocabulary()</code> became slightly more efficient - it performs less unnecessary computations;</li>
</ol>

<h2 id="transformers">Transformers</h2>

<p>All transformers renamed, now all starts with <code>transform_*</code> (this was done for more convenient work with autocompletion):</p>

<ul>
<li><code>transform_binary</code></li>
<li><code>transform_tfidf</code></li>
<li><code>transform_tf</code></li>
<li><code>transform_filter_commons</code> still useful, even with some intersection with <code>prune_vocabulary</code></li>
</ul>

<p>The following example demonstrates new pipeline with many text2vec features:
(note how flexible text2vec can be! thanks to functional style)</p>

<pre><code class="language-r">library(text2vec)
# for stemming
library(SnowballC)
data(&quot;movie_review&quot;)

stem_tokenizer &lt;- function(x, tokenizer = word_tokenizer) {
  x %&gt;% 
    tokenizer %&gt;% 
    # poerter stemmer
    lapply(wordStem, 'en')
}

# create list of stemmed tokens
# each element of list is a representation of original document
tokens &lt;- movie_review$review %&gt;% 
  tolower %&gt;% 
  stem_tokenizer

# keep document ids in dtm and corpus!
names(tokens) &lt;- movie_review$id

stopwords &lt;- c(&quot;i&quot;, &quot;me&quot;, &quot;my&quot;, &quot;myself&quot;, &quot;we&quot;, &quot;our&quot;, &quot;ours&quot;, &quot;ourselves&quot;, &quot;you&quot;, &quot;your&quot;, &quot;yours&quot;) %&gt;%
  # here we stem stopwords, because stop-words filtering would be performed after tokenization!
  wordStem('en')

it &lt;- itoken(tokens)
vocab &lt;- create_vocabulary(it, ngram = c(1L, 1L), stopwords = stopwords)

# remove common and uncommon words  
pruned_vocab = prune_vocabulary(vocab,  term_count_min = 5, doc_proportion_max = 0.5)
str(pruned_vocab)
</code></pre>

<pre class="output">
List of 4
 $ vocab         :Classes 'data.table' and 'data.frame':    9595 obs. of  3 variables:
  ..$ terms       : chr [1:9595] &quot;fiorentino&quot; &quot;bfg&quot; &quot;tadashi&quot; &quot;kabei&quot; ...
  ..$ terms_counts: int [1:9595] 5 8 5 5 11 5 6 10 6 8 ...
  ..$ doc_counts  : int [1:9595] 1 1 1 1 1 1 1 1 1 1 ...
  ..- attr(*, &quot;.internal.selfref&quot;)=&lt;externalptr&gt; 
 $ ngram         : Named int [1:2] 1 1
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;ngram_min&quot; &quot;ngram_max&quot;
 $ document_count: int 5000
 $ stopwords     : chr [1:11] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; ...
 - attr(*, &quot;class&quot;)= chr &quot;text2vec_vocabulary&quot;
</pre>

<p>One important note. In current R realization, <strong>iterators are mutable</strong>. So at this point our iterator is empty:</p>

<pre><code class="language-r">try(iterators::nextElem(it))
</code></pre>

<p>So before <code>corpus</code> / <code>dtm</code> / <code>tcm</code> construction we need to reinitialise it. Here we create <code>dtm</code> directly:</p>

<pre><code class="language-r">it &lt;- itoken(tokens)
v_vectorizer &lt;- vocab_vectorizer(pruned_vocab)
dtm &lt;- create_dtm(it, v_vectorizer)
# check  that dtm keep documents names/ids as rownames
head(rownames(dtm))
</code></pre>

<pre class="output">
[1] &quot;5814_8&quot; &quot;2381_9&quot; &quot;7759_3&quot; &quot;3630_4&quot; &quot;9495_8&quot; &quot;8196_8&quot;
</pre>

<pre><code class="language-r">identical(rownames(dtm), movie_review$id)
</code></pre>

<pre class="output">
[1] TRUE
</pre>

<p>Or <code>tcm</code>:</p>

<pre><code class="language-r">it &lt;- itoken(tokens)
cooccurence_vectorizer &lt;- vocab_vectorizer(pruned_vocab, grow_dtm = FALSE, skip_grams_window = 5L)
tcm &lt;- create_tcm(it, cooccurence_vectorizer)
</code></pre>

<p>Old-style simultenious vectorization and collection of cooccurence statistics:</p>

<pre><code class="language-r">it &lt;- itoken(tokens)
v_vectorizer &lt;- vocab_vectorizer(pruned_vocab, grow_dtm = TRUE, skip_grams_window = 5L)
corpus &lt;- create_corpus(it, v_vectorizer)
dtm &lt;- get_dtm(corpus)
tcm &lt;- get_tcm(corpus)
</code></pre>

<p>Another option is to use <code>hash_vectorizer</code>. Procedure is the same:</p>

<pre><code class="language-r"># create hash vectorizer for unigrams and bigrams
h_vectorizer &lt;- hash_vectorizer(hash_size = 2 ^ 16, ngram = c(1L, 2L))
it &lt;- itoken(tokens)
dtm &lt;- create_dtm(it, h_vectorizer)
</code></pre>

<h2 id="parallel-mode">Parallel mode</h2>

<p>Now <code>create_dtm</code>, <code>create_tcm</code>, <code>create_vocabulary</code> take advantage of multicore machines and do it in transparent manner. In contrast to GloVe fitting which uses low-level thread parallelism via <code>RcppParallel</code>, other functions use standart R high-level parallelism on top of <code>foreach</code> package. They are flexible and can use diffrent parallel backends - <code>doParallel</code>, <code>doRedis</code>, etc. But <strong>user should remember that such high-level parallelism can involve significant overhead</strong>.</p>

<p>Only two things user should perform manually to take advantage of multicore machine:</p>

<ol>
<li>prepare splits of input data in a form of <code>list</code> of <code>itoken</code> iterators.</li>
<li>register parallel backend</li>
</ol>

<p>Here is simple example with timings:</p>

<pre><code class="language-r">N_WORKERS &lt;- 2
library(doParallel)
library(microbenchmark)
registerDoParallel(N_WORKERS)

# &quot;jobs&quot; is a list of itoken iterators!
N_SPLITS &lt;- 2
jobs &lt;- tokens %&gt;% 
  split_into(N_SPLITS) %&gt;% 
  lapply(itoken)

# performance comparison between serial and parallel versions

# vocabulary creation
microbenchmark(
  vocab_serial &lt;- create_vocabulary(itoken(tokens), stopwords = stopwords), 
  vocab_parallel &lt;- create_vocabulary(jobs, stopwords = stopwords), 
  times = 1
)
</code></pre>

<pre class="output">
Unit: milliseconds
                                                                     expr
 vocab_serial &lt;- create_vocabulary(itoken(tokens), stopwords = stopwords)
         vocab_parallel &lt;- create_vocabulary(jobs, stopwords = stopwords)
      min       lq     mean   median       uq      max neval
 382.0348 382.0348 382.0348 382.0348 382.0348 382.0348     1
 254.0068 254.0068 254.0068 254.0068 254.0068 254.0068     1
</pre>

<pre><code class="language-r"># dtm vocabulary vectorization
v_vectorizer &lt;- vocab_vectorizer(vocab_serial)
# dtm feature hashing
h_vectorizer &lt;- hash_vectorizer()
# tcm vectorization
tcm_vectorizer &lt;- vocab_vectorizer(vocab_serial, grow_dtm = T, skip_grams_window = 5)

microbenchmark(
  vocab_dtm_serial &lt;- create_dtm(itoken(tokens), vectorizer = v_vectorizer),
  vocab_dtm_parallel &lt;- create_dtm(jobs, vectorizer = v_vectorizer),
  hash_dtm_serial &lt;- create_dtm(itoken(tokens), vectorizer = h_vectorizer),
  hash_dtm_parallel &lt;- create_dtm(jobs, vectorizer = h_vectorizer), 
  tcm_serial &lt;- create_dtm(itoken(tokens), vectorizer = tcm_vectorizer), 
  tcm_parallel &lt;- create_dtm(jobs, vectorizer = tcm_vectorizer), 
  times = 1
)
</code></pre>

<pre class="output">
Unit: milliseconds
                                                                      expr
 vocab_dtm_serial &lt;- create_dtm(itoken(tokens), vectorizer = v_vectorizer)
         vocab_dtm_parallel &lt;- create_dtm(jobs, vectorizer = v_vectorizer)
  hash_dtm_serial &lt;- create_dtm(itoken(tokens), vectorizer = h_vectorizer)
          hash_dtm_parallel &lt;- create_dtm(jobs, vectorizer = h_vectorizer)
     tcm_serial &lt;- create_dtm(itoken(tokens), vectorizer = tcm_vectorizer)
             tcm_parallel &lt;- create_dtm(jobs, vectorizer = tcm_vectorizer)
       min        lq      mean    median        uq       max neval
 1054.9643 1054.9643 1054.9643 1054.9643 1054.9643 1054.9643     1
  697.1996  697.1996  697.1996  697.1996  697.1996  697.1996     1
 1234.3570 1234.3570 1234.3570 1234.3570 1234.3570 1234.3570     1
  592.7327  592.7327  592.7327  592.7327  592.7327  592.7327     1
 3136.1603 3136.1603 3136.1603 3136.1603 3136.1603 3136.1603     1
 1780.9763 1780.9763 1780.9763 1780.9763 1780.9763 1780.9763     1
</pre>

<p>As you can see, speedup is not perfect. This happened because, <strong>R&rsquo;s high-level parallelism has significant overhead on small tasks. On larger tasks you can expect almost linear speedup</strong>!</p>

<h2 id="bonus-how-fast-is-fast">Bonus: how fast is fast?</h2>

<p>On 16-core machine I was able to perform vectorization (unigrams) of english wikipedia (13 gb of text, 4M of documents) in 2.5 minutes using hash vectorizer and in 6 minutes using vocabulary vectorizer. Timings include time spent for reading from disk! Resulted <code>dtm</code> was about 13gb and at peak R processes consumes about 30gb of RAM. (Try to do it with <strong>any</strong> other R package or python module).</p>

<p>Here is code:</p>

<pre><code class="language-r">library(text2vec)
library(data.table)

library(doParallel)
registerDoParallel(16)

start &lt;- Sys.time()
# tab-separated wikipedia &quot;article_title \t article_body&quot;
# article_body is &quot;single splace&quot; separated

reader &lt;- function(x) {
  fread(x, sep = '\t', header = F, select = 2, colClasses = rep('character', 2))[[1]]
}

# each file is roughly 100mb
fls &lt;- list.files(&quot;~/datasets/enwiki_splits/&quot;, full.names = T)

# jobs are simply list of itoken iterators. Each element is separate job in a separate process.
# after finish the will be efficiently combined. (especially efficiently in case of `dgTMatrix`)
jobs &lt;- fls %&gt;% 
  # combine files into 64 groups, so we will have 64 jobs
  split_into(64) %&gt;% 
  lapply(function(x) x %&gt;% ifiles(reader_function = reader) %&gt;% itoken)

# alternatively can process each file as separate job
# jobs &lt;- lapply(fls, function(x) x %&gt;% ifiles(reader_function = reader) %&gt;% itoken)

v &lt;- create_vocabulary(jobs) %&gt;% 
  prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.2)

dtm &lt;- create_dtm(jobs, vocab_vectorizer(v), type = 'dgTMatrix')

finish &lt;- Sys.time()
</code></pre>

<h3 id="updates">Updates</h3>

<ul>
<li><strong>updated 2016-03-31</strong>: a few synatax improvements, to be consistenr with <a href="http://adv-r.had.co.nz/Style.html">Hadley&rsquo;s style guide</a> - all function names are verbs:

<ul>
<li><em><code>vocabulary</code> -&gt; <code>create_vocabulary</code></em></li>
<li><em><code>tranformer_*</code>-&gt; <code>tranform_*</code></em></li>
</ul></li>
</ul>

</div>


      <footer>
  <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
      
  
    <nav><ul class="pager">
    
        <li class="previous">
          <a href="/post/r-read-hdfs/" title="Read from hdfs with R. Brief overview of SparkR.">
            <span aria-hidden="true">&larr;</span>Previous
          </a>
        </li>
    

    
      <li class="next">
        <a href="/post/text2vec-0-4/" title="text2vec 0.4">
            Next <span aria-hidden="true">&rarr;</span>
        </a>
      </li>
    
    </ul> </nav>
  


</div>

  <div class="col-xs-12 col-sm-12 col-md-9 col-lg-9">
  
<div id="disqus_thread"></div>
<script type="text/javascript">
  (function() {
    
    
    if (window.location.hostname == "localhost")
      return;

    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';
    dsq.src = '//dselivanov.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</div>

</footer>

    </div>
    
      <div class="col-xs-12 col-sm-12 col-md-3 col-lg-3">
        <div>
  

    <div class="section">
      <header><div class="title"><b>Latest Posts</b></div></header>
      <div class="content">
        <ul>
        
          <li>
          <a href="/post/2017-02-07-large-data-feature-hashing-and-online-learning-part-2/">Fitting logistic regression on 100gb dataset on a laptop</a>
          </li>
        
          <li>
          <a href="/post/2017-01-27-lessons-learned-from-outbrain-click-prediction-kaggle-competition/">Large data, feature hashing and online learning</a>
          </li>
        
          <li>
          <a href="/post/text2vec-0-4/">text2vec 0.4</a>
          </li>
        
          <li>
          <a href="/post/text2vec-0-3/">text2vec 0.3</a>
          </li>
        
          <li>
          <a href="/post/r-read-hdfs/">Read from hdfs with R. Brief overview of SparkR.</a>
          </li>
        
          <li>
          <a href="/post/fast-parallel-async-adagrad/">text2vec GloVe implementation details</a>
          </li>
        
          <li>
          <a href="/post/glove-enwiki/">GloVe vs word2vec revisited.</a>
          </li>
        
          <li>
          <a href="/post/text2vec/">Analyzing texts with text2vec package</a>
          </li>
        
          <li>
          <a href="/post/r-and-mssql/">Working with MS SQL server on non-windows systems</a>
          </li>
        
          <li>
          <a href="/post/installing-cuda-toolkit-and-gputools/">Installing cuda toolkit and related R packages</a>
          </li>
        
        </ul>
      </div>
    </div>

    
      
      
      <div class="section taxonomies">
        <header><div class="title"><b>tag</b></div></header>

        <div class="content">
          <ul>
            <li><a href="/tags/text2vec">text2vec</a></li><li><a href="/tags/data_table">data_table</a></li><li><a href="/tags/big_data">big_data</a></li><li><a href="/tags/glove">glove</a></li><li><a href="/tags/hashing_trick">hashing_trick</a></li><li><a href="/tags/kaggle">kaggle</a></li><li><a href="/tags/online_learning">online_learning</a></li><li><a href="/tags/setup">setup</a></li><li><a href="/tags/gpgpu">gpgpu</a></li><li><a href="/tags/lsh">lsh</a></li>
          </ul>
        </div>
      </div>
      
    
      
      
    

</div>

      </div>
    
  </div>
</div>
      
<footer class="footer hidden-print">
  <div class="container">
    <div class="row">
        <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
           <div class="pull-left">
  <a class="toplink" href="javascript:" id="return-to-top">back to top</a>
</div>
<div class="pull-right">

</div>

        </div>
        <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12 text-center">
              
    
<div class="container footline">
    <small>
</small>
</div>


    


        </div>
    </div>
  </div>
</footer>

    

<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.4.9/angular.min.js"></script>
<script src="/js/popover/angular-storage.min.js"></script>


<script type="text/javascript">
  (function() {
    
    
    if (window.location.hostname == "localhost")
      return;

    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';
    dsq.src = '//dselivanov.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('#return-to-top').click(function() {      
    $('body,html').animate({
        scrollTop : 0                       
    }, 500);
});
</script>


<script src="/js/highlight.pack.js"></script>
<script src="/js/site.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<script>
  var _gaq=[['_setAccount','UA-56994099-1'],['_trackPageview']];
  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
  s.parentNode.insertBefore(g,s)}(document,'script'));
</script>

<script>
var ENABLE_POPOVER = ""; 
var EXPIRE_COOKIE = ""; 
var SHOW_MODAL_TIMEOUT = ""; 
var MOUSE_LEAVE = ""; 
var MODAL_SIZE = ""; 
var POST_URL = ""; 
var SIGNUP_HEADER = "";
var HEADER_IMAGE = "";
var IMG_DESCRIPTION = "";
var SIGNUP_TEXT = "";
var INPUT_PLACEHOLDER = "";
var SUBMIT_BUTTON = "";
var SUCCESS_MESSAGE = "";
var ERROR_MESSAGE = "";
var OPTIN = "";
var COOKIE_NAME = "";
</script>
<script src="/js/popover/angular-modal-service.min.js"></script>
<script src="/js/angular-ismobile.min.js"></script>
<script src="/js/popover/popover.min.js"></script>

    
    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\[','\]']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
      }
    });
    </script>
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>

  </body>
</html>

