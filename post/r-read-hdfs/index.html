<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Read from hdfs with R. Brief overview of SparkR.  &middot; Data Science notes</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="" />

<meta name="keywords" content="Spark, SparkR, data_table, ">


<meta property="og:title" content="Read from hdfs with R. Brief overview of SparkR.  &middot; Data Science notes ">
<meta property="og:site_name" content="Data Science notes"/>
<meta property="og:url" content="/post/r-read-hdfs/" />
<meta property="og:locale" content="en-us">


<meta property="og:type" content="article" />
<meta property="og:description" content=""/>
<meta property="og:article:published_time" content="2016-02-20T00:00:00Z" />
<meta property="og:article:modified_time" content="2016-02-20T00:00:00Z" />

  
    
<meta property="og:article:tag" content="Spark">
    
<meta property="og:article:tag" content="SparkR">
    
<meta property="og:article:tag" content="data_table">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@dselivanov_" />
<meta name="twitter:creator" content="@dselivanov_" />
<meta name="twitter:title" content="Read from hdfs with R. Brief overview of SparkR." />
<meta name="twitter:description" content="" />
<meta name="twitter:url" content="/post/r-read-hdfs/" />
<meta name="twitter:domain" content="/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Read from hdfs with R. Brief overview of SparkR.",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2016-02-20",
    "description": "",
    "wordCount":  1173 
  }
</script>



<link rel="canonical" href="/post/r-read-hdfs/" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
<link rel="icon" href="/favicon.png">
<meta name="generator" content="Hugo 0.18" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.2/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->



    <link rel="stylesheet" href="/css/bootswatch/paper/bootstrap.min.css">


<link rel="stylesheet" href="/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/style.css">


  <link rel="stylesheet" href="/css/highlight/default.css">


</head>
<body class="map[name:paper]" data-ng-app="myapp" data-ng-controller="MyController" data-ng-mouseleave="MouseLeave($event)">
    <header id="main-header">
  <nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        
          
          <a class="navbar-brand-img" href="/">
            <img alt="" src="">
            
          </a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav navbar-right">
            
            
            <li class="">

              <a href="/" >
                
                Blog
              </a>
            </li>
            
            <li class="">

              <a href="/projects" >
                
                Projects
              </a>
            </li>
            
            <li class="">

              <a href="/about" >
                
                About
              </a>
            </li>
            
          </ul>
        </div>
        
      </div>
    </nav>
  </header>


<div class="container">
  <div class="row">
    <div class="col-sm-9">
      <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
  <div class="text-center">

    <h1>Read from hdfs with R. Brief overview of SparkR.
</h1>

    <div class="metas">
<small>
  <i class="fa fa-calendar"></i>
  <time datetime="2016-02-20">20 Feb, 2016</time>
</small>


  <small>
    &middot; by Dmitriy Selivanov
  
  &middot; Read in about 6 min
  &middot; (1173 words)
</small>


<div class="margin-10">
  <i class="fa fa-tags"></i>
  
  <a href="/tags/spark" class="label label-primary">Spark</a>
  
  <a href="/tags/sparkr" class="label label-primary">SparkR</a>
  
  <a href="/tags/data_table" class="label label-primary">data_table</a>
  


</div>
<br>
</div>

  </div>
</div>

      <div class="content">
  

<p><em>Disclaimer: originally I planned to write post about R functions/packages which allow to read data from hdfs (with benchmarks), but in the end it became more like an overview of SparkR capabilities.</em></p>

<p>Nowadays working with &ldquo;big data&rdquo; almost always means working with hadoop ecosystem. A few years ago this also meant that you also would have to be a good java programmer to work in such environment - even simple <a href="https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0">word count</a> program took several dozens of lines of code. But 2-3 years ago things changed - thanks to <a href="http://spark.apache.org/">Apache Spark</a> with its concise (but powerful!) functional-style API. It is written in Scala, but also has java, python and recently <strong>R</strong> APIs.</p>

<h1 id="spark">Spark</h1>

<p>I started to use Spark more than 2 years ago (and used it a lot). In most cases I use scala because</p>

<ul>
<li>JVM native</li>
<li>the only fully featured - RDD level API, MLlib, GraphX, etc.</li>
<li>nice REPL</li>
<li>scala is well suited for data munging - good tradeoff between complexity and efficiency.</li>
</ul>

<p>During this period I tried several times SparkR, but until version 1.6 it had too many rough edges. Starting from 1.6 it became a really useful tool for simple manipulations on spark data frames. Unfortunately we still do not have R user defined functions, so sparkR functionality is limited to built-in functions.
Common pipelene for data scientist can be the following:</p>

<ol>
<li>read data from hdfs</li>
<li>do some data wrangling (join/filter/etc.)</li>
<li>optionally take subset/sample and collect data to local R session for exploratory analysis and fitting models.</li>
</ol>

<p>Lets have a closer look into these steps.</p>

<h3 id="reading-data-from-hdfs">Reading data from hdfs</h3>

<p>Files in hdfs are usually stored in the following formats:</p>

<ol>
<li>plain txt/csv/json files</li>
<li><a href="https://wiki.apache.org/hadoop/SequenceFile">sequence files</a>. You can think of them as serialized java objects. In recent years became less popular. Also they are not portable (need custom readers), so I do not find them interesting for this post.</li>
<li><a href="https://avro.apache.org/">avro</a> (row-based)</li>
<li><a href="https://parquet.apache.org/">paruqet</a> (column-based)</li>
<li><a href="https://orc.apache.org/">orc</a> (column-based)</li>
</ol>

<p>Good news is that Spark (and SparkR!) can read <code>json</code>, <code>parquet</code>, <code>orc</code> with built-in <code>read.df</code> function and <code>csv</code>, <code>avro</code> with <code>read.df</code> and <a href="http://spark-packages.org/package/databricks/spark-avro">spark-avro</a>, <a href="http://spark-packages.org/package/databricks/spark-csv">spark-csv</a> spark packages.</p>

<h3 id="data-wrangling">Data wrangling</h3>

<p>SparkR allows to perform dplyr-style manipulations on spark data frames. See official <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">DataFrame</a> and <a href="http://spark.apache.org/docs/latest/sparkr.html">SparkR</a> documents for details. Also I would like to highlight, that package provides quite comprehensive set of methods for manipulations on spark data frames including functions for:</p>

<ul>
<li>data frames <code>join</code>, <code>filter</code>, <code>group_by</code>, <code>sample</code></li>
<li>date / time manipulations</li>
<li>string manipulations, regular expressions</li>
<li>general math / statistical functions like <code>sin</code>, <code>cos</code>, <code>mean</code>, etc.</li>
</ul>

<p>See full list of functions in <a href="http://spark.apache.org/docs/latest/api/R/index.html">package documentation</a>.</p>

<h3 id="collecting-data-to-local-r-session">Collecting data to local R session</h3>

<p>However if you need to perform more complex manipulations to fit some model, you may need to collect data to local R session (and take a sample if size is too big). And here you can be unpleasantly surprised - collecting even small 50mb data frame can take minutes (see example below). Current mechanism of serialization / deserealization between R and JVM was designed primarily for exchanging meta-information (like function calls), not data. See this <a href="https://issues.apache.org/jira/browse/SPARK-12635">JIRA tikcket</a> for details. Hopefully this issue will be fixed in the next release.</p>

<h1 id="examples-and-timings">Examples and timings</h1>

<p>First of all we need several things to be installed:</p>

<ol>
<li>hadoop. I have it installed at <code>/opt/hadoop-2.6.0</code>.</li>
<li>Spark and SparkR - just download prebuilded version and unpack it. <code>/opt/spark-1.6.0-bin-hadoop2.6</code> in my case.</li>
</ol>

<h3 id="setting-up-sparkr-on-yarn">Setting up SparkR on YARN</h3>

<p>At work I have YARN cluster and client machine with Rstudio Server from which I usually work. To make SparkR work with Rstudio Server you should set up several system variables - <code>SPARK_HOME</code>, <code>YARN_CONF_DIR</code>, etc. You can follow <a href="http://spark.apache.org/docs/latest/sparkr.html#starting-up-from-rstudio">official manual</a>, but doing this each time makes me sad. The simpler way is to add this variables to  <code>~/.Renviron.site</code> or <code>{R_HOME}/etc/Renviron.site</code>(for system-wide oprions) files. Here are my configs:</p>

<pre><code>SPARK_HOME=/opt/spark-1.6.0-bin-hadoop2.6
R_LIBS_SITE=${R_LIBS_SITE}:${SPARK_HOME}/R/lib
YARN_CONF_DIR=/opt/hadoop-2.6.0/etc/hadoop
JAVA_HOME=/usr/java/jdk170_64_45
</code></pre>

<h2 id="reading-from-hdfs-to-local-r-session">Reading from hdfs to local R session</h2>

<p>For becnhmarks we will generate small data frame with 1M rows:</p>

<pre><code class="language-r">N &lt;- 1e6
k &lt;- 1e4
df &lt;- data.frame(V_int = sample(N, N, replace = T), 
                 V_num_1 = sample(N, N, replace = T) + 0.1,
                 V_num_2 = sample(N, N, replace = T) + 0.2, 
                 V_char_1 = rep(paste0('factor_1_', 1:k), each = N/k),
                 V_char_2 = rep(paste0('factor_2_', 1:k), each = N/k)
                 )
format(object.size(df), 'Mb')
# &quot;27.9 Mb&quot;
</code></pre>

<p>Now we will save it to disk and copy to hdfs:</p>

<pre><code class="language-r">write.table(df, 'test_spark.csv', sep = ',', row.names = F, col.names = F)
# command to call hadoop
hadoop_cmd &lt;- &quot;/opt/hadoop-2.6.0/bin/hadoop&quot;
# copy csv from to hdfs
system2(hadoop_cmd, &quot;fs -copyFromLocal test_spark.csv /user/dmitry.selivanov/csv/&quot;)
</code></pre>

<p>Now lets try to read it with SparkR and collect to local R session:</p>

<pre><code class="language-r">library(SparkR)

spark_env = list('spark.executor.memory' = '4g', 
                 'spark.executor.instances' = '4', 
                 'spark.executor.cores' = '4',
                 'spark.driver.memory' = '4g')

# here we use spark-csv package
# since I don't have direct internet access on my Rstudio server machine I uploded needed jars myself
# note that this is not assemlbed &quot;fat&quot; jar, so we also need commons-csv class
sc &lt;- sparkR.init(master = &quot;yarn-client&quot;, appName = &quot;SparkR&quot;, sparkEnvir = spark_env, 
                  sparkJars=c(&quot;/home/dmitry.selivanov/packages/spark-csv_2.10-1.3.0.jar&quot;, 
                              &quot;/home/dmitry.selivanov/packages/commons-csv-1.2.jar&quot;))

sqlContext &lt;- sparkRHive.init(sc)

sdf &lt;- read.df(sqlContext, path = &quot;/user/dmitry.selivanov/csv/test_spark.csv&quot;, 
               source = &quot;com.databricks.spark.csv&quot;, inferSchema = &quot;true&quot;)
# first we cache
cache(sdf)
# and trigger computation
nrow(sdf)
# now our sdf is materialized and in RAM
# lets collect it to local df
system.time(sdf_local &lt;- collect(sdf))
# 130.927
</code></pre>

<p>more than 2 minutes! So at least until next release we should avoid using <code>collect</code> for any medium to large size data frames.</p>

<h2 id="alternatives">Alternatives</h2>

<h2 id="data-table-https-github-com-rdatatable-data-table"><a href="https://github.com/Rdatatable/data.table">data.table</a></h2>

<p>Here my favourite package comes in - <strong>data.table</strong> and <code>fread</code> function. I believe many of <code>data.table</code> users don&rsquo;t know, that <code>fread</code> input can be not only a file name, but also a unix pipe!</p>

<pre><code class="language-r">library(data.table)
system.time(sdf_local &lt;- fread(paste(hadoop_cmd, &quot;fs -text /user/dmitry.selivanov/csv/test_spark.csv&quot;)))
# 4.005
</code></pre>

<p>This takes only 4 seconds! Antother great thing is that <code>fs -text</code> command can automatically choose codec for uncompressing files:</p>

<pre><code class="language-r"># write file splitted into 16 chunks
repartition(sdf, 16)
# save it with gzip compression
write.df(sdf, 
         path = &quot;/user/dmitry.selivanov/csv/test_spark&quot;,
         source = &quot;com.databricks.spark.csv&quot;, 
         codec = &quot;org.apache.hadoop.io.compress.GzipCodec&quot;)
# read entire directory with gzipped files
system.time(sdf_local &lt;- fread(paste(hadoop_cmd, &quot;fs -text /user/dmitry.selivanov/csv/test_spark/*&quot;)))
# 4.784
</code></pre>

<h2 id="dataconnector-https-github-com-vertica-r-dataconnector"><a href="https://github.com/vertica/r-dataconnector">dataconnector</a></h2>

<p>One drawback of <code>data.table::fread</code> is that it can parse only flat files. Spark data frames can consists of nested columns (like R data frame with columns of type <code>list</code>). For such (usually rare) cases we can save data frame in <code>orc</code> format and then read it with <code>dataconnector::orc2dataframe</code> function.</p>

<p><code>dataconnector</code> is new package developed HP Vertica Analytics Team (probably initially for working with <a href="https://github.com/vertica/DistributedR">DistributedR</a>) and unfortunately not well known yet. But it is incredibly useful - it allows to:</p>

<ol>
<li>read <code>orc</code> and <code>csv</code> files from local file system or hdfs. Hope eventually we will also obtain <a href="https://github.com/vertica/r-dataconnector/issues/1">parquet support</a>;</li>
<li>write arbitrary R objects directly to hdfs;</li>
</ol>

<p>Another nice thing is that it <strong>doesn&rsquo;t requre hadoop and java/RJava</strong>!</p>

<pre><code class="language-r"># save df in orc format
# create 
conf &lt;- 
'{
  &quot;webhdfsPort&quot;: YOUR_webhdfsPort_HERE,
  &quot;hdfsPort&quot;: YOUR_hdfsPort_HERE,
  &quot;hdfsHost&quot;: &quot;YOUR_HOST_HERE&quot;,
  &quot;hdfsUser&quot;: &quot;YOUR_USERNAME_HERE&quot;
}'
system.time(sdf_local &lt;- orc2dataframe(
  &quot;hdfs:///user/dmitry.selivanov/csv/test_spark/part-r-00000-1eb57e5d-2a98-489d-b2b0-a5dc44e9538b.orc&quot;, 
  hdfsConfigurationStr = conf))
# 6.330
</code></pre>

<h2 id="other-options">other options</h2>

<ul>
<li><a href="https://github.com/RevolutionAnalytics/rhdfs">rhdfs</a> and <a href="https://github.com/RevolutionAnalytics/ravro">ravro</a> packages by RevolutionAnalytics. Never tried, so can&rsquo;t say anything.</li>
<li><code>h2o::h2o.importFile</code>, but it can be tricky to set up <a href="h2o.ai">h2o</a> in hdfs-client mode.</li>
</ul>

<p><strong>What tools you use? Please, share your experience in comments.</strong></p>

</div>


      <footer>
  <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
      
  
    <nav><ul class="pager">
    
        <li class="previous">
          <a href="/post/fast-parallel-async-adagrad/" title="text2vec GloVe implementation details">
            <span aria-hidden="true">&larr;</span>Previous
          </a>
        </li>
    

    
      <li class="next">
        <a href="/post/text2vec-0-3/" title="text2vec 0.3">
            Next <span aria-hidden="true">&rarr;</span>
        </a>
      </li>
    
    </ul> </nav>
  


</div>

  <div class="col-xs-12 col-sm-12 col-md-9 col-lg-9">
  
<div id="disqus_thread"></div>
<script type="text/javascript">
  (function() {
    
    
    if (window.location.hostname == "localhost")
      return;

    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';
    dsq.src = '//dselivanov.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</div>

</footer>

    </div>
    
      <div class="col-xs-12 col-sm-12 col-md-3 col-lg-3">
        <div>
  

    <div class="section">
      <header><div class="title"><b>Latest Posts</b></div></header>
      <div class="content">
        <ul>
        
          <li>
          <a href="/post/2017-02-07-large-data-feature-hashing-and-online-learning-part-2/">Fitting logistic regression on 100gb dataset on a laptop</a>
          </li>
        
          <li>
          <a href="/post/2017-01-27-lessons-learned-from-outbrain-click-prediction-kaggle-competition/">Large data, feature hashing and online learning</a>
          </li>
        
          <li>
          <a href="/post/text2vec-0-4/">text2vec 0.4</a>
          </li>
        
          <li>
          <a href="/post/text2vec-0-3/">text2vec 0.3</a>
          </li>
        
          <li>
          <a href="/post/r-read-hdfs/">Read from hdfs with R. Brief overview of SparkR.</a>
          </li>
        
          <li>
          <a href="/post/fast-parallel-async-adagrad/">text2vec GloVe implementation details</a>
          </li>
        
          <li>
          <a href="/post/glove-enwiki/">GloVe vs word2vec revisited.</a>
          </li>
        
          <li>
          <a href="/post/text2vec/">Analyzing texts with text2vec package</a>
          </li>
        
          <li>
          <a href="/post/r-and-mssql/">Working with MS SQL server on non-windows systems</a>
          </li>
        
          <li>
          <a href="/post/installing-cuda-toolkit-and-gputools/">Installing cuda toolkit and related R packages</a>
          </li>
        
        </ul>
      </div>
    </div>

    
      
      
      <div class="section taxonomies">
        <header><div class="title"><b>tag</b></div></header>

        <div class="content">
          <ul>
            <li><a href="/tags/text2vec">text2vec</a></li><li><a href="/tags/data_table">data_table</a></li><li><a href="/tags/big_data">big_data</a></li><li><a href="/tags/glove">glove</a></li><li><a href="/tags/hashing_trick">hashing_trick</a></li><li><a href="/tags/kaggle">kaggle</a></li><li><a href="/tags/online_learning">online_learning</a></li><li><a href="/tags/setup">setup</a></li><li><a href="/tags/gpgpu">gpgpu</a></li><li><a href="/tags/lsh">lsh</a></li>
          </ul>
        </div>
      </div>
      
    
      
      
    

</div>

      </div>
    
  </div>
</div>
      
<footer class="footer hidden-print">
  <div class="container">
    <div class="row">
        <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
           <div class="pull-left">
  <a class="toplink" href="javascript:" id="return-to-top">back to top</a>
</div>
<div class="pull-right">

</div>

        </div>
        <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12 text-center">
              
    
<div class="container footline">
    <small>
</small>
</div>


    


        </div>
    </div>
  </div>
</footer>

    

<script src="/js/jquery.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.4.9/angular.min.js"></script>
<script src="/js/popover/angular-storage.min.js"></script>


<script type="text/javascript">
  (function() {
    
    
    if (window.location.hostname == "localhost")
      return;

    var dsq = document.createElement('script'); dsq.async = true; dsq.type = 'text/javascript';
    dsq.src = '//dselivanov.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  $('#return-to-top').click(function() {      
    $('body,html').animate({
        scrollTop : 0                       
    }, 500);
});
</script>


<script src="/js/highlight.pack.js"></script>
<script src="/js/site.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<script>
  var _gaq=[['_setAccount','UA-56994099-1'],['_trackPageview']];
  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
  s.parentNode.insertBefore(g,s)}(document,'script'));
</script>

<script>
var ENABLE_POPOVER = ""; 
var EXPIRE_COOKIE = ""; 
var SHOW_MODAL_TIMEOUT = ""; 
var MOUSE_LEAVE = ""; 
var MODAL_SIZE = ""; 
var POST_URL = ""; 
var SIGNUP_HEADER = "";
var HEADER_IMAGE = "";
var IMG_DESCRIPTION = "";
var SIGNUP_TEXT = "";
var INPUT_PLACEHOLDER = "";
var SUBMIT_BUTTON = "";
var SUCCESS_MESSAGE = "";
var ERROR_MESSAGE = "";
var OPTIN = "";
var COOKIE_NAME = "";
</script>
<script src="/js/popover/angular-modal-service.min.js"></script>
<script src="/js/angular-ismobile.min.js"></script>
<script src="/js/popover/popover.min.js"></script>

    
    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\[','\]']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
             extensions: ["AMSmath.js", "AMSsymbols.js"] }
      }
    });
    </script>
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>

  </body>
</html>

